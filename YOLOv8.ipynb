{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFRndJtlc8I_"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "!pip install scikit-learn-intelex\n",
        "!pip install intel-numpy\n",
        "!pip install intel-scipy\n",
        "!pip install intel-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es50KGCKj12M",
        "outputId": "cd4acd30-5fc2-478c-f14c-10bb4178bdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.55 🚀 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 28.9/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC-Ceo3QkDaE",
        "outputId": "5657a541-1f9e-4dc6-d42c-9505e1feea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Data.zip -d Dataset"
      ],
      "metadata": {
        "id": "f6-ylto9kjtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('/content/best.pt')\n",
        "results = model.train(data='/content/Dataset/data.yaml', epochs=10)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIpFSBg6kDWP",
        "outputId": "9ac34fca-a1e2-4e31-a287-2d3de7b5d4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.55 🚀 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=/content/best.pt, data=/content/Dataset/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753652  ultralytics.nn.modules.Detect                [12, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3013188 parameters, 3013172 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Dataset/train/labels... 777 images, 0 backgrounds, 0 corrupt: 100%|██████████| 777/777 [00:00<00:00, 1820.42it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/15_jpg.rf.069df255fb08e53a153a999ceead012b.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/16_jpg.rf.c4bc45a8814ba644223f52954ad113e2.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/19_jpg.rf.9965fd956b5268aa2093c9e942e629d1.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/1_jpg.rf.5ccfc69f3a8c7009dd1e9709edfd0e9c.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/20_jpg.rf.289ba468d6cf72a52dc58356ea2bbeaa.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/31_jpg.rf.3399574f174f54a96ba43fdd72bb26f3.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/31_jpg.rf.b9fef2089325da14d9d7a94fd77b8e44.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/3_jpg.rf.3876954dfd50061d5f4547dd506e7c20.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/3_jpg.rf.956d91f6f224f6604fac941b78581289.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/3_jpg.rf.b0f7ef02c147628334e7f909cb5a7603.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/44_jpg.rf.86e64fa574bf2b1f997b8cc87932e2dc.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/5_jpg.rf.5a6c9b0410cf4bf0678b12dc3d6d4f39.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/6_jpg.rf.fe7911a4296730ac03f44ef45dceff01.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/Banana-12_jpg.rf.7e0fda5e379d5e0a630593201939b09a.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/image_2023-03-19_125347418_png.rf.ceb95d76148d7105b171666b7aab13d1.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/image_2023-03-19_134231768_png.rf.a9234f54837d1e778310072c4961227b.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/image_2023-03-19_135331191_png.rf.de7ac116a2ea76206673028ebfd21c40.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/image_2023-03-19_140427526_png.rf.84dfb3efcd4bdc47dfe3ffe66053bb05.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/Dataset/train/images/image_2023-03-19_140712146_png.rf.3e3d2b1fae1899c6a02dc55d866821de.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Dataset/valid/labels... 74 images, 0 backgrounds, 0 corrupt: 100%|██████████| 74/74 [00:00<00:00, 1901.18it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Dataset/valid/labels.cache\n",
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      2.54G     0.3476     0.3839     0.9317         54        640: 100%|██████████| 49/49 [00:24<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]\n",
            "                   all         74        291      0.933      0.881       0.95      0.816\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      3.66G     0.3042     0.3318     0.8912         58        640: 100%|██████████| 49/49 [00:20<00:00,  2.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "                   all         74        291       0.93      0.858      0.931      0.788\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      3.66G      0.328     0.3655     0.9068         52        640: 100%|██████████| 49/49 [00:20<00:00,  2.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]\n",
            "                   all         74        291      0.884      0.873      0.898      0.746\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      3.66G     0.3546     0.3907     0.9266         53        640: 100%|██████████| 49/49 [00:19<00:00,  2.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]\n",
            "                   all         74        291       0.83      0.827      0.887      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      3.66G     0.3661      0.418     0.9337         64        640: 100%|██████████| 49/49 [00:19<00:00,  2.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]\n",
            "                   all         74        291      0.898      0.831      0.922      0.754\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      3.66G      0.363     0.3985      0.925         55        640: 100%|██████████| 49/49 [00:20<00:00,  2.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "                   all         74        291      0.884      0.815      0.916      0.723\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      3.66G     0.3443     0.3815     0.9216         62        640: 100%|██████████| 49/49 [00:20<00:00,  2.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
            "                   all         74        291      0.915      0.882      0.917      0.756\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      3.66G     0.3383     0.3746     0.9223         59        640: 100%|██████████| 49/49 [00:19<00:00,  2.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]\n",
            "                   all         74        291      0.945      0.884      0.945      0.777\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      3.66G     0.3179     0.3531     0.9106         67        640: 100%|██████████| 49/49 [00:19<00:00,  2.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.17it/s]\n",
            "                   all         74        291      0.925      0.853      0.932      0.771\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      3.66G     0.3064     0.3344     0.9076         62        640: 100%|██████████| 49/49 [00:19<00:00,  2.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n",
            "                   all         74        291      0.923       0.88      0.934      0.781\n",
            "\n",
            "10 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.55 🚀 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3007988 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n",
            "                   all         74        291      0.933      0.881       0.95      0.816\n",
            "                 Apple         74         81      0.794      0.827      0.884      0.651\n",
            "                Banana         74         13      0.966      0.846      0.925       0.79\n",
            "                Burger         74         13      0.981          1      0.995      0.858\n",
            "               Chapati         74         14      0.795      0.832      0.893      0.784\n",
            "         Chicken Curry         74          3      0.935          1      0.995      0.895\n",
            "                 Fries         74          7          1       0.89      0.995      0.855\n",
            "                  Idli         74         36          1        0.6      0.883      0.823\n",
            "                 Pizza         74          4      0.944          1      0.995      0.853\n",
            "                  Rice         74          9      0.972          1      0.995      0.784\n",
            "                  Soda         74          5      0.964          1      0.995      0.922\n",
            "                Tomato         74         69      0.949      0.816       0.96      0.815\n",
            "                  Vada         74         37      0.892      0.757      0.887      0.761\n",
            "Speed: 2.5ms preprocess, 4.4ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Ultralytics YOLOv8.0.55 🚀 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3007988 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Dataset/valid/labels.cache... 74 images, 0 backgrounds, 0 corrupt: 100%|██████████| 74/74 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:05<00:00,  1.01s/it]\n",
            "                   all         74        291      0.933      0.881       0.95      0.816\n",
            "                 Apple         74         81      0.795      0.827      0.884      0.653\n",
            "                Banana         74         13      0.967      0.846      0.925       0.79\n",
            "                Burger         74         13      0.982          1      0.995      0.858\n",
            "               Chapati         74         14      0.795      0.831      0.893      0.784\n",
            "         Chicken Curry         74          3      0.936          1      0.995      0.895\n",
            "                 Fries         74          7          1       0.89      0.995      0.855\n",
            "                  Idli         74         36          1        0.6      0.883      0.823\n",
            "                 Pizza         74          4      0.945          1      0.995      0.853\n",
            "                  Rice         74          9      0.972          1      0.995      0.784\n",
            "                  Soda         74          5      0.964          1      0.995      0.922\n",
            "                Tomato         74         69      0.949      0.815       0.96      0.815\n",
            "                  Vada         74         37      0.892      0.757      0.887      0.763\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val7\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}